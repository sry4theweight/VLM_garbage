{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training Description LLM\n",
        "\n",
        "Обучение языковой модели для генерации описаний по выходу CV моделей.\n",
        "\n",
        "**Архитектура:**\n",
        "```\n",
        "CV Output (JSON) → LLM → Natural Language Description\n",
        "```\n",
        "\n",
        "**Вход LLM:** Структурированные данные от детектора и классификатора сцен\n",
        "```json\n",
        "{\"detections\": [{\"label\": \"plastic\", \"confidence\": 0.92}, ...], \"scene\": {\"class\": \"grass\", \"confidence\": 0.95}}\n",
        "```\n",
        "\n",
        "**Выход LLM:** Естественное описание\n",
        "```\n",
        "\"There are 2 plastic bottles and 1 metal can scattered on the grassy area.\"\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Установка зависимостей\n",
        "# !pip install transformers datasets accelerate peft\n",
        "\n",
        "import json\n",
        "import random\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import torch\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Загрузка CV моделей и генерация данных\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from vlm_annotation.ensemble_detector import EnsembleDetector\n",
        "from train_scene_yolo import SceneClassifierYOLO\n",
        "\n",
        "detector = EnsembleDetector(\n",
        "    yolo_model_path=\"models/yolo/yolov8x/best.pt\",\n",
        "    detr_model_path=\"models/rt-detr/rt-detr-101/m\",\n",
        "    detr_processor_path=\"models/rt-detr/rt-detr-101/p\",\n",
        "    conf_threshold=0.5\n",
        ")\n",
        "\n",
        "scene_classifier = None\n",
        "if Path(\"models/scene_classifier_yolo.pt\").exists():\n",
        "    scene_classifier = SceneClassifierYOLO(\"models/scene_classifier_yolo.pt\")\n",
        "    print(\"Scene classifier loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_cv_output(image_path):\n",
        "    \"\"\"Получение структурированного выхода CV моделей\"\"\"\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    \n",
        "    detections = detector.detect(image)\n",
        "    \n",
        "    detection_summary = []\n",
        "    for det in detections:\n",
        "        detection_summary.append({\n",
        "            \"label\": det[\"label\"],\n",
        "            \"confidence\": round(det[\"confidence\"], 2)\n",
        "        })\n",
        "    \n",
        "    scene = {\"class\": \"unknown\", \"confidence\": 0.0}\n",
        "    if scene_classifier:\n",
        "        scene_result = scene_classifier.predict(image)\n",
        "        scene = {\n",
        "            \"class\": scene_result[\"class\"],\n",
        "            \"confidence\": round(scene_result[\"confidence\"], 2)\n",
        "        }\n",
        "    \n",
        "    return {\n",
        "        \"detections\": detection_summary,\n",
        "        \"scene\": scene\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DESCRIPTION_TEMPLATES = [\n",
        "    \"There {verb} {garbage_desc}{scene_desc}.\",\n",
        "    \"{garbage_desc} {verb} visible{scene_desc}.\",\n",
        "    \"The image shows {garbage_desc}{scene_desc}.\",\n",
        "    \"{garbage_desc} can be seen{scene_desc}.\",\n",
        "    \"I can see {garbage_desc}{scene_desc}.\",\n",
        "]\n",
        "\n",
        "GARBAGE_PHRASES = {\n",
        "    \"plastic\": [\"plastic waste\", \"plastic items\", \"plastic debris\", \"plastic bottles\"],\n",
        "    \"glass\": [\"glass bottles\", \"glass items\", \"broken glass\", \"glass waste\"],\n",
        "    \"metal\": [\"metal cans\", \"metal waste\", \"metallic debris\", \"aluminum cans\"],\n",
        "    \"paper\": [\"paper waste\", \"cardboard\", \"paper debris\", \"discarded paper\"],\n",
        "    \"organic\": [\"organic waste\", \"food scraps\", \"biodegradable waste\", \"organic matter\"],\n",
        "}\n",
        "\n",
        "SCENE_PHRASES = {\n",
        "    \"grass\": [\" on the grass\", \" in a grassy area\", \" on green grass\", \" scattered on the lawn\"],\n",
        "    \"sandy\": [\" on sandy ground\", \" on the beach\", \" in sandy terrain\", \" on the sand\"],\n",
        "    \"rocky\": [\" on rocky terrain\", \" among rocks\", \" on rocky ground\", \" between stones\"],\n",
        "    \"marshy\": [\" in marshy area\", \" in wetlands\", \" in swampy terrain\", \" near marsh\"],\n",
        "}\n",
        "\n",
        "def generate_description(cv_output):\n",
        "    \"\"\"Генерация естественного описания по CV выходу\"\"\"\n",
        "    detections = cv_output[\"detections\"]\n",
        "    scene = cv_output[\"scene\"]\n",
        "    \n",
        "    if not detections:\n",
        "        if scene[\"class\"] != \"unknown\" and scene[\"confidence\"] >= 0.8:\n",
        "            scene_phrase = random.choice(SCENE_PHRASES.get(scene[\"class\"], [\"\"]))\n",
        "            return f\"No garbage detected{scene_phrase}.\"\n",
        "        return \"No garbage detected in this image.\"\n",
        "    \n",
        "    counts = {}\n",
        "    for det in detections:\n",
        "        label = det[\"label\"]\n",
        "        counts[label] = counts.get(label, 0) + 1\n",
        "    \n",
        "    garbage_parts = []\n",
        "    for label, count in counts.items():\n",
        "        phrase = random.choice(GARBAGE_PHRASES.get(label, [label]))\n",
        "        if count == 1:\n",
        "            garbage_parts.append(f\"1 {phrase.rstrip('s')}\" if phrase.endswith('s') else f\"1 {phrase}\")\n",
        "        else:\n",
        "            garbage_parts.append(f\"{count} {phrase}\")\n",
        "    \n",
        "    if len(garbage_parts) == 1:\n",
        "        garbage_desc = garbage_parts[0]\n",
        "    elif len(garbage_parts) == 2:\n",
        "        garbage_desc = f\"{garbage_parts[0]} and {garbage_parts[1]}\"\n",
        "    else:\n",
        "        garbage_desc = \", \".join(garbage_parts[:-1]) + f\", and {garbage_parts[-1]}\"\n",
        "    \n",
        "    scene_desc = \"\"\n",
        "    if scene[\"class\"] != \"unknown\" and scene[\"confidence\"] >= 0.8:\n",
        "        scene_desc = random.choice(SCENE_PHRASES.get(scene[\"class\"], [\"\"]))\n",
        "    \n",
        "    total = sum(counts.values())\n",
        "    verb = \"is\" if total == 1 else \"are\"\n",
        "    \n",
        "    template = random.choice(DESCRIPTION_TEMPLATES)\n",
        "    return template.format(verb=verb, garbage_desc=garbage_desc, scene_desc=scene_desc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_training_dataset(image_dirs, output_file, max_images=None, variations_per_image=3):\n",
        "    \"\"\"Создание датасета для обучения LLM\"\"\"\n",
        "    \n",
        "    image_paths = []\n",
        "    for img_dir in image_dirs:\n",
        "        img_dir = Path(img_dir)\n",
        "        if img_dir.exists():\n",
        "            image_paths.extend(list(img_dir.glob(\"*.jpg\")))\n",
        "            image_paths.extend(list(img_dir.glob(\"*.png\")))\n",
        "    \n",
        "    if max_images:\n",
        "        random.shuffle(image_paths)\n",
        "        image_paths = image_paths[:max_images]\n",
        "    \n",
        "    print(f\"Processing {len(image_paths)} images...\")\n",
        "    \n",
        "    dataset = []\n",
        "    for img_path in tqdm(image_paths):\n",
        "        try:\n",
        "            cv_output = get_cv_output(str(img_path))\n",
        "            \n",
        "            for _ in range(variations_per_image):\n",
        "                description = generate_description(cv_output)\n",
        "                dataset.append({\n",
        "                    \"input\": json.dumps(cv_output),\n",
        "                    \"output\": description,\n",
        "                    \"image\": img_path.name\n",
        "                })\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {img_path}: {e}\")\n",
        "            continue\n",
        "    \n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(dataset, f, indent=2, ensure_ascii=False)\n",
        "    \n",
        "    print(f\"Created dataset with {len(dataset)} examples\")\n",
        "    return dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Создаем датасет\n",
        "train_dataset = create_training_dataset(\n",
        "    image_dirs=[\n",
        "        \"data/1206-data/train\",\n",
        "        \"data/1206-data/valid\",\n",
        "    ],\n",
        "    output_file=\"data/llm_train_data.json\",\n",
        "    max_images=2000,\n",
        "    variations_per_image=3\n",
        ")\n",
        "\n",
        "print(\"\\nПримеры данных:\")\n",
        "for i, sample in enumerate(train_dataset[:3]):\n",
        "    print(f\"\\n--- Sample {i+1} ---\")\n",
        "    print(f\"Input: {sample['input'][:80]}...\")\n",
        "    print(f\"Output: {sample['output']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Подготовка и обучение модели T5\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import (\n",
        "    T5ForConditionalGeneration, \n",
        "    T5Tokenizer,\n",
        "    Seq2SeqTrainer,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    DataCollatorForSeq2Seq\n",
        ")\n",
        "from datasets import Dataset\n",
        "\n",
        "MODEL_NAME = \"google/flan-t5-small\"\n",
        "OUTPUT_DIR = \"models/description_llm\"\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n",
        "model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
        "\n",
        "print(f\"Model: {MODEL_NAME}\")\n",
        "print(f\"Parameters: {model.num_parameters():,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "PROMPT_TEMPLATE = \"\"\"Describe the garbage detected based on this CV output:\n",
        "{input}\n",
        "Description:\"\"\"\n",
        "\n",
        "with open(\"data/llm_train_data.json\", 'r', encoding='utf-8') as f:\n",
        "    raw_data = json.load(f)\n",
        "\n",
        "random.shuffle(raw_data)\n",
        "split_idx = int(len(raw_data) * 0.9)\n",
        "train_data = raw_data[:split_idx]\n",
        "val_data = raw_data[split_idx:]\n",
        "\n",
        "print(f\"Train: {len(train_data)}, Val: {len(val_data)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_function(examples):\n",
        "    inputs = [PROMPT_TEMPLATE.format(input=inp) for inp in examples[\"input\"]]\n",
        "    targets = examples[\"output\"]\n",
        "    \n",
        "    model_inputs = tokenizer(inputs, max_length=256, truncation=True, padding=\"max_length\")\n",
        "    labels = tokenizer(targets, max_length=128, truncation=True, padding=\"max_length\")\n",
        "    \n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "train_ds = Dataset.from_list(train_data)\n",
        "val_ds = Dataset.from_list(val_data)\n",
        "\n",
        "train_ds = train_ds.map(preprocess_function, batched=True, remove_columns=train_ds.column_names)\n",
        "val_ds = val_ds.map(preprocess_function, batched=True, remove_columns=val_ds.column_names)\n",
        "\n",
        "print(f\"Train dataset: {len(train_ds)}, Val dataset: {len(val_ds)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=500,\n",
        "    learning_rate=3e-4,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=2,\n",
        "    save_steps=500,\n",
        "    logging_steps=100,\n",
        "    predict_with_generate=True,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, padding=True)\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=val_ds,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Starting training...\")\n",
        "trainer.train()\n",
        "\n",
        "trainer.save_model(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "print(f\"Model saved to {OUTPUT_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Тестирование модели\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = T5ForConditionalGeneration.from_pretrained(OUTPUT_DIR)\n",
        "tokenizer = T5Tokenizer.from_pretrained(OUTPUT_DIR)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "print(f\"Model loaded on {device}\")\n",
        "\n",
        "def generate_description_llm(cv_output):\n",
        "    \"\"\"Генерация описания с помощью LLM\"\"\"\n",
        "    prompt = PROMPT_TEMPLATE.format(input=json.dumps(cv_output))\n",
        "    \n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=256, truncation=True)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_length=128,\n",
        "            num_beams=4,\n",
        "            early_stopping=True,\n",
        "            no_repeat_ngram_size=2\n",
        "        )\n",
        "    \n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Тест на реальных изображениях\n",
        "test_images = list(Path(\"data/1206-data/test\").glob(\"*.jpg\"))[:5]\n",
        "\n",
        "print(\"Testing on sample images:\\n\")\n",
        "for img_path in test_images:\n",
        "    cv_output = get_cv_output(str(img_path))\n",
        "    description = generate_description_llm(cv_output)\n",
        "    \n",
        "    print(f\"Image: {img_path.name}\")\n",
        "    print(f\"CV: {len(cv_output['detections'])} detections, scene={cv_output['scene']['class']}\")\n",
        "    print(f\"LLM: {description}\")\n",
        "    print(\"-\" * 50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Сохраняем конфиг для использования в VLM\n",
        "config = {\n",
        "    \"model_path\": OUTPUT_DIR,\n",
        "    \"model_type\": \"t5\",\n",
        "    \"prompt_template\": PROMPT_TEMPLATE,\n",
        "    \"max_input_length\": 256,\n",
        "    \"max_output_length\": 128,\n",
        "    \"num_beams\": 4\n",
        "}\n",
        "\n",
        "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "with open(f\"{OUTPUT_DIR}/llm_config.json\", 'w') as f:\n",
        "    json.dump(config, f, indent=2)\n",
        "\n",
        "print(f\"Config saved to {OUTPUT_DIR}/llm_config.json\")\n",
        "print(\"\\nModel ready for use in VLM_Complete_LLM.py\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
