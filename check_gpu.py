"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ GPU –∏ —Å–∏—Å—Ç–µ–º–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
"""

import torch
import sys
import os


def get_conda_info():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ Conda –æ–∫—Ä—É–∂–µ–Ω–∏–∏"""
    conda_env = os.environ.get('CONDA_DEFAULT_ENV', None)
    conda_prefix = os.environ.get('CONDA_PREFIX', None)
    return conda_env, conda_prefix


def check_gpu():
    """–ü–æ–ª–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ GPU –∏ —Å–∏—Å—Ç–µ–º–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
    print("=" * 60)
    print("–ü–†–û–í–ï–†–ö–ê GPU –ò –°–ò–°–¢–ï–ú–ù–û–ô –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–ò")
    print("=" * 60)
    
    # Conda –æ–∫—Ä—É–∂–µ–Ω–∏–µ
    conda_env, conda_prefix = get_conda_info()
    if conda_env:
        print(f"\nüêç Conda –æ–∫—Ä—É–∂–µ–Ω–∏–µ: {conda_env}")
        print(f"   –ü—É—Ç—å: {conda_prefix}")
    else:
        print("\nüêç Conda –æ–∫—Ä—É–∂–µ–Ω–∏–µ: –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ")
    
    # Python –≤–µ—Ä—Å–∏—è
    print(f"\nüìå Python –≤–µ—Ä—Å–∏—è: {sys.version}")
    print(f"üìå PyTorch –≤–µ—Ä—Å–∏—è: {torch.__version__}")
    
    if torch.cuda.is_available():
        gpu_count = torch.cuda.device_count()
        print(f"\n‚úÖ CUDA –¥–æ—Å—Ç—É–ø–Ω–∞!")
        print(f"   CUDA –≤–µ—Ä—Å–∏—è: {torch.version.cuda}")
        print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ GPU: {gpu_count}")
        
        for i in range(gpu_count):
            props = torch.cuda.get_device_properties(i)
            gpu_name = props.name
            gpu_memory = props.total_memory / (1024**3)  # GB
            
            print(f"\n   üñ•Ô∏è  GPU {i}: {gpu_name}")
            print(f"   - –û–±—â–∞—è –ø–∞–º—è—Ç—å: {gpu_memory:.2f} GB")
            print(f"   - Compute Capability: {props.major}.{props.minor}")
            print(f"   - –ú—É–ª—å—Ç–∏–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–≤: {props.multi_processor_count}")
        
        # cuDNN
        if hasattr(torch.backends, 'cudnn'):
            print(f"\n   cuDNN –≤–µ—Ä—Å–∏—è: {torch.backends.cudnn.version()}")
            print(f"   cuDNN –≤–∫–ª—é—á–µ–Ω: {torch.backends.cudnn.enabled}")
        
        # –¢–µ–∫—É—â–µ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏
        current_device = torch.cuda.current_device()
        allocated = torch.cuda.memory_allocated(current_device) / (1024**3)
        reserved = torch.cuda.memory_reserved(current_device) / (1024**3)
        print(f"\n   –¢–µ–∫—É—â–µ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ GPU {current_device}:")
        print(f"   - –í—ã–¥–µ–ª–µ–Ω–æ: {allocated:.2f} GB")
        print(f"   - –ó–∞—Ä–µ–∑–µ—Ä–≤–∏—Ä–æ–≤–∞–Ω–æ: {reserved:.2f} GB")
        
        # –¢–µ—Å—Ç —Ä–∞–±–æ—Ç—ã GPU
        print("\n   üß™ –¢–µ—Å—Ç —Ä–∞–±–æ—Ç—ã GPU...")
        try:
            x = torch.randn(1000, 1000, device='cuda')
            y = torch.randn(1000, 1000, device='cuda')
            z = torch.matmul(x, y)
            torch.cuda.synchronize()
            print("   ‚úÖ –¢–µ—Å—Ç –ø—Ä–æ–π–¥–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∞: {e}")
        
        print("\n" + "=" * 60)
        print("‚úÖ GPU –≥–æ—Ç–æ–≤ –∫ –æ–±—É—á–µ–Ω–∏—é!")
        print("=" * 60)
        return True
    else:
        print("\n‚ùå CUDA –ù–ï –¥–æ—Å—Ç—É–ø–Ω–∞!")
        print("   –û–±—É—á–µ–Ω–∏–µ –±—É–¥–µ—Ç –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç—å –Ω–∞ CPU (–æ—á–µ–Ω—å –º–µ–¥–ª–µ–Ω–Ω–æ)")
        
        print("\n   –í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã:")
        print("   1. –ù–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã NVIDIA –¥—Ä–∞–π–≤–µ—Ä—ã")
        print("   2. PyTorch —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –±–µ–∑ –ø–æ–¥–¥–µ—Ä–∂–∫–∏ CUDA")
        print("   3. –ù–µ—Ç —Å–æ–≤–º–µ—Å—Ç–∏–º–æ–π NVIDIA GPU")
        
        print("\n   üìã –î–ª—è —É—Å—Ç–∞–Ω–æ–≤–∫–∏ PyTorch —Å CUDA —á–µ—Ä–µ–∑ Conda:")
        print("   # CUDA 11.8:")
        print("   conda install pytorch torchvision pytorch-cuda=11.8 -c pytorch -c nvidia")
        print("\n   # CUDA 12.1:")
        print("   conda install pytorch torchvision pytorch-cuda=12.1 -c pytorch -c nvidia")
        print("\n   # –ò–ª–∏ —á–µ—Ä–µ–∑ pip –≤ Conda –æ–∫—Ä—É–∂–µ–Ω–∏–∏:")
        print("   pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118")
        
        print("\n" + "=" * 60)
        print("‚ùå GPU –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
        print("=" * 60)
        return False


def check_memory_for_model(model_size_gb: float = 14.0):
    """
    –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ—Å—Ç–∏ –ø–∞–º—è—Ç–∏ GPU –¥–ª—è –º–æ–¥–µ–ª–∏
    
    Args:
        model_size_gb: –ø—Ä–∏–º–µ—Ä–Ω—ã–π —Ä–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏ –≤ GB (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 14GB –¥–ª—è LLaVA-7B)
    """
    if not torch.cuda.is_available():
        print("GPU –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω")
        return False
    
    props = torch.cuda.get_device_properties(0)
    total_memory = props.total_memory / (1024**3)
    
    print(f"\nüìä –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–∞–º—è—Ç–∏ –¥–ª—è –º–æ–¥–µ–ª–∏:")
    print(f"   –î–æ—Å—Ç—É–ø–Ω–∞—è –ø–∞–º—è—Ç—å GPU: {total_memory:.2f} GB")
    print(f"   –¢—Ä–µ–±—É–µ–º–∞—è –ø–∞–º—è—Ç—å (–ø—Ä–∏–º–µ—Ä–Ω–æ): {model_size_gb:.2f} GB")
    
    if total_memory >= model_size_gb:
        print(f"   ‚úÖ –ü–∞–º—è—Ç–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ!")
        return True
    else:
        print(f"   ‚ö†Ô∏è  –ü–∞–º—è—Ç–∏ –º–æ–∂–µ—Ç –Ω–µ —Ö–≤–∞—Ç–∏—Ç—å!")
        print(f"   –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:")
        print(f"   - –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ LoRA –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –∫ –ø–∞–º—è—Ç–∏")
        print(f"   - –£–º–µ–Ω—å—à–∏—Ç–µ batch_size")
        print(f"   - –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ gradient checkpointing")
        return False


if __name__ == "__main__":
    has_gpu = check_gpu()
    
    if has_gpu:
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–∞–º—è—Ç–∏ –¥–ª—è LLaVA-7B (~14GB VRAM)
        check_memory_for_model(14.0)

