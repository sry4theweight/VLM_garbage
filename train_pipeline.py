"""
–ü–æ–ª–Ω—ã–π pipeline –æ–±—É—á–µ–Ω–∏—è VLM:
1. –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –≤ —Ñ–æ—Ä–º–∞—Ç LLaVA
2. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/val (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
3. –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
"""

import argparse
import json
from pathlib import Path

from vlm_training.convert_to_llava_format import convert_to_llava_format
from vlm_training.split_data import split_data


def run_pipeline(
    annotation_file: str,
    output_dir: str,
    base_model: str = "llava-hf/llava-1.5-7b-hf",
    train_ratio: float = 0.9,
    batch_size: int = 2,
    num_epochs: int = 3,
    learning_rate: float = 2e-5,
    skip_training: bool = False
):
    """
    –ü–æ–ª–Ω—ã–π pipeline –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –∏ –æ–±—É—á–µ–Ω–∏—è VLM
    
    Args:
        annotation_file: –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π (–æ—Ç annotate_roboflow_dataset.py)
        output_dir: –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        base_model: –±–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å LLaVA
        train_ratio: –¥–æ–ª—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        batch_size: —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
        num_epochs: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö
        learning_rate: —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
        skip_training: –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ (—Ç–æ–ª—å–∫–æ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö)
    """
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    print("=" * 60)
    print("PIPELINE –û–ë–£–ß–ï–ù–ò–Ø VLM")
    print("=" * 60)
    
    # –®–∞–≥ 1: –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ —Ñ–æ—Ä–º–∞—Ç LLaVA
    print("\nüìù –®–∞–≥ 1: –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –≤ —Ñ–æ—Ä–º–∞—Ç LLaVA...")
    llava_file = output_path / "llava_data.json"
    convert_to_llava_format(annotation_file, str(llava_file))
    
    # –®–∞–≥ 2: –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/val
    print("\nüìä –®–∞–≥ 2: –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ train/val...")
    train_file = output_path / "train.json"
    val_file = output_path / "val.json"
    
    split_data(
        input_file=str(llava_file),
        train_file=str(train_file),
        val_file=str(val_file),
        train_ratio=train_ratio,
        val_ratio=1 - train_ratio
    )
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    with open(train_file, 'r', encoding='utf-8') as f:
        train_data = json.load(f)
    with open(val_file, 'r', encoding='utf-8') as f:
        val_data = json.load(f)
    
    print(f"\nüìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö:")
    print(f"   Train samples: {len(train_data)}")
    print(f"   Val samples: {len(val_data)}")
    
    if skip_training:
        print("\n‚è≠Ô∏è  –û–±—É—á–µ–Ω–∏–µ –ø—Ä–æ–ø—É—â–µ–Ω–æ (--skip_training)")
        print(f"\n‚úÖ –î–∞–Ω–Ω—ã–µ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã –≤: {output_path}")
        print(f"   - LLaVA —Ñ–æ—Ä–º–∞—Ç: {llava_file}")
        print(f"   - Train: {train_file}")
        print(f"   - Val: {val_file}")
        return
    
    # –®–∞–≥ 3: –û–±—É—á–µ–Ω–∏–µ
    print("\nüöÄ –®–∞–≥ 3: –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è...")
    print(f"   –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å: {base_model}")
    print(f"   Batch size: {batch_size}")
    print(f"   Epochs: {num_epochs}")
    print(f"   Learning rate: {learning_rate}")
    
    # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –∑–¥–µ—Å—å, —á—Ç–æ–±—ã –Ω–µ –∑–∞–≥—Ä—É–∂–∞—Ç—å torch –µ—Å–ª–∏ —Ç–æ–ª—å–∫–æ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    from vlm_training.train_vlm import train_vlm
    
    model_output_dir = output_path / "model"
    
    train_vlm(
        train_data_file=str(train_file),
        val_data_file=str(val_file),
        output_dir=str(model_output_dir),
        base_model=base_model,
        use_lora=True,
        batch_size=batch_size,
        learning_rate=learning_rate,
        num_epochs=num_epochs
    )
    
    print("\n" + "=" * 60)
    print("‚úÖ Pipeline –∑–∞–≤–µ—Ä—à—ë–Ω!")
    print(f"   –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {model_output_dir}")
    print("=" * 60)


def main():
    parser = argparse.ArgumentParser(description="Pipeline –æ–±—É—á–µ–Ω–∏—è VLM")
    parser.add_argument(
        "--annotation_file",
        type=str,
        default="data/vlm_annotations/annotations.json",
        help="–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π"
    )
    parser.add_argument(
        "--output_dir",
        type=str,
        default="data/vlm_training",
        help="–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"
    )
    parser.add_argument(
        "--base_model",
        type=str,
        default="llava-hf/llava-1.5-7b-hf",
        help="–ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å LLaVA"
    )
    parser.add_argument(
        "--train_ratio",
        type=float,
        default=0.9,
        help="–î–æ–ª—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 0.9)"
    )
    parser.add_argument(
        "--batch_size",
        type=int,
        default=2,
        help="–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 2 –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏)"
    )
    parser.add_argument(
        "--num_epochs",
        type=int,
        default=3,
        help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö"
    )
    parser.add_argument(
        "--learning_rate",
        type=float,
        default=2e-5,
        help="–°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è"
    )
    parser.add_argument(
        "--skip_training",
        action="store_true",
        help="–¢–æ–ª—å–∫–æ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö, –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è"
    )
    
    args = parser.parse_args()
    
    run_pipeline(
        annotation_file=args.annotation_file,
        output_dir=args.output_dir,
        base_model=args.base_model,
        train_ratio=args.train_ratio,
        batch_size=args.batch_size,
        num_epochs=args.num_epochs,
        learning_rate=args.learning_rate,
        skip_training=args.skip_training
    )


if __name__ == "__main__":
    main()

