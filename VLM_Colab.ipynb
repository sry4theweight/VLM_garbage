{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üóëÔ∏è VLM –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –º—É—Å–æ—Ä–∞ (–ê–Ω—Å–∞–º–±–ª—å + LLM)\n",
        "\n",
        "**–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:**\n",
        "```\n",
        "–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ ‚Üí –í–∞—à –ê–Ω—Å–∞–º–±–ª—å (YOLO+RT-DETR) ‚Üí –î–µ—Ç–µ–∫—Ü–∏–∏ ‚Üí LLM ‚Üí –û—Ç–≤–µ—Ç\n",
        "```\n",
        "\n",
        "–≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≤–∞—à–∏ –æ–±—É—á–µ–Ω–Ω—ã–µ –¥–µ—Ç–µ–∫—Ç–æ—Ä—ã –∫–∞–∫ \"–≥–ª–∞–∑–∞\" VLM.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∏ –∫–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ\n",
        "!git lfs install\n",
        "!git clone https://github.com/sry4theweight/VLM_garbage.git\n",
        "%cd VLM_garbage\n",
        "\n",
        "# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π\n",
        "!pip install -q transformers peft accelerate ultralytics supervision tqdm pillow roboflow\n",
        "\n",
        "# –ü—Ä–æ–≤–µ—Ä–∫–∞ GPU\n",
        "import torch\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. –°–∫–∞—á–∞—Ç—å –¥–∞—Ç–∞—Å–µ—Ç Roboflow\n",
        "!python download_roboflow_dataset.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π\n",
        "import sys\n",
        "sys.path.append('.')\n",
        "import torch\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from pathlib import Path\n",
        "import random\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "\n",
        "# –ó–∞–≥—Ä—É–∑–∫–∞ –∞–Ω—Å–∞–º–±–ª—è –¥–µ—Ç–µ–∫—Ç–æ—Ä–æ–≤ (–≤–∞—à–∏ –º–æ–¥–µ–ª–∏)\n",
        "from vlm_annotation.ensemble_detector import EnsembleDetector\n",
        "\n",
        "detector = EnsembleDetector(\n",
        "    yolo_model_path=\"models/yolo/yolov8x/best.pt\",\n",
        "    detr_model_path=\"models/rt-detr/rt-detr-101/m\",\n",
        "    detr_processor_path=\"models/rt-detr/rt-detr-101/p\",\n",
        "    conf_threshold=0.5\n",
        ")\n",
        "print(\"‚úÖ –ê–Ω—Å–∞–º–±–ª—å –¥–µ—Ç–µ–∫—Ç–æ—Ä–æ–≤ –∑–∞–≥—Ä—É–∂–µ–Ω!\")\n",
        "\n",
        "# –ó–∞–≥—Ä—É–∑–∫–∞ BLIP –¥–ª—è –æ–ø–∏—Å–∞–Ω–∏—è —Å—Ü–µ–Ω—ã\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
        "blip_model = BlipForConditionalGeneration.from_pretrained(\n",
        "    \"Salesforce/blip-image-captioning-large\"\n",
        ").to(device)\n",
        "blip_model.eval()\n",
        "print(\"‚úÖ BLIP –∑–∞–≥—Ä—É–∂–µ–Ω –¥–ª—è –æ–ø–∏—Å–∞–Ω–∏—è —Å—Ü–µ–Ω—ã!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. –°–æ–±–∏—Ä–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –ë–ï–ó –ê–£–ì–ú–ï–ù–¢–ê–¶–ò–ô (valid + test)\n",
        "valid_dir = Path(\"data/roboflow_dataset/valid\")\n",
        "test_dir = Path(\"data/roboflow_dataset/test\")\n",
        "\n",
        "all_images = list(valid_dir.glob(\"*.jpg\")) + list(test_dir.glob(\"*.jpg\"))\n",
        "print(f\"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(all_images)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (valid + test, –±–µ–∑ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–π)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5. –ö–ª–∞—Å—Å VLM: –ê–Ω—Å–∞–º–±–ª—å + BLIP –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –æ–ø–∏—Å–∞–Ω–∏—è\n",
        "COLORS = {'glass': 'green', 'plastic': 'red', 'metal': 'cyan', 'paper': 'yellow', 'organic': 'lime'}\n",
        "\n",
        "def describe_scene(image):\n",
        "    \"\"\"–û–ø–∏—Å–∞–Ω–∏–µ —Å—Ü–µ–Ω—ã —Å –ø–æ–º–æ—â—å—é BLIP (—Ç—Ä–∞–≤–∞, —É–ª–∏—Ü–∞, –ø–µ—Å–æ–∫ –∏ —Ç.–¥.)\"\"\"\n",
        "    inputs = blip_processor(image, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        out = blip_model.generate(**inputs, max_length=50)\n",
        "    return blip_processor.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "def describe_garbage_location(image, detections):\n",
        "    \"\"\"–û–ø–∏—Å–∞–Ω–∏–µ –≥–¥–µ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –º—É—Å–æ—Ä\"\"\"\n",
        "    if not detections:\n",
        "        return \"No garbage detected in this image.\"\n",
        "    \n",
        "    # –ü–æ–ª—É—á–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ —Å—Ü–µ–Ω—ã\n",
        "    scene = describe_scene(image)\n",
        "    \n",
        "    # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç—å –∏–∑ –æ–ø–∏—Å–∞–Ω–∏—è\n",
        "    surfaces = ['grass', 'ground', 'sand', 'pavement', 'road', 'floor', 'table', 'street', 'beach', 'soil']\n",
        "    found_surface = None\n",
        "    for s in surfaces:\n",
        "        if s in scene.lower():\n",
        "            found_surface = s\n",
        "            break\n",
        "    \n",
        "    # –ü–æ–¥—Å—á—ë—Ç –æ–±—ä–µ–∫—Ç–æ–≤\n",
        "    class_counts = {}\n",
        "    for det in detections:\n",
        "        label = det['label']\n",
        "        class_counts[label] = class_counts.get(label, 0) + 1\n",
        "    \n",
        "    # –§–æ—Ä–º–∏—Ä—É–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ\n",
        "    items = []\n",
        "    for label, count in class_counts.items():\n",
        "        if count == 1:\n",
        "            items.append(f\"a {label} item\")\n",
        "        else:\n",
        "            items.append(f\"{count} {label} items\")\n",
        "    \n",
        "    garbage_str = \", \".join(items[:-1]) + \" and \" + items[-1] if len(items) > 1 else items[0]\n",
        "    \n",
        "    if found_surface:\n",
        "        return f\"There is {garbage_str} on the {found_surface}.\"\n",
        "    else:\n",
        "        return f\"There is {garbage_str} in this image. Scene: {scene}\"\n",
        "\n",
        "def answer_question(image, detections, question):\n",
        "    \"\"\"–û—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å –æ –º—É—Å–æ—Ä–µ\"\"\"\n",
        "    q_lower = question.lower()\n",
        "    \n",
        "    # –ü–æ–¥—Å—á—ë—Ç\n",
        "    class_counts = {}\n",
        "    for det in detections:\n",
        "        label = det['label']\n",
        "        class_counts[label] = class_counts.get(label, 0) + 1\n",
        "    total = len(detections)\n",
        "    \n",
        "    # –í–æ–ø—Ä–æ—Å –æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ\n",
        "    if \"how many\" in q_lower:\n",
        "        for cls in ['glass', 'plastic', 'metal', 'paper', 'organic']:\n",
        "            if cls in q_lower:\n",
        "                count = class_counts.get(cls, 0)\n",
        "                return f\"There {'is' if count == 1 else 'are'} {count} {cls} object{'s' if count != 1 else ''}.\"\n",
        "        return f\"There are {total} garbage objects in total.\"\n",
        "    \n",
        "    # –í–æ–ø—Ä–æ—Å –æ –Ω–∞–ª–∏—á–∏–∏\n",
        "    if \"is there\" in q_lower or \"are there\" in q_lower:\n",
        "        for cls in ['glass', 'plastic', 'metal', 'paper', 'organic']:\n",
        "            if cls in q_lower:\n",
        "                count = class_counts.get(cls, 0)\n",
        "                if count > 0:\n",
        "                    return f\"Yes, there {'is' if count == 1 else 'are'} {count} {cls} object{'s' if count != 1 else ''}.\"\n",
        "                else:\n",
        "                    return f\"No, there is no {cls} in this image.\"\n",
        "    \n",
        "    # –í–æ–ø—Ä–æ—Å \"—á—Ç–æ –Ω–∞ –∫–∞—Ä—Ç–∏–Ω–∫–µ\"\n",
        "    if \"what\" in q_lower:\n",
        "        return describe_garbage_location(image, detections)\n",
        "    \n",
        "    # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é - –æ–ø–∏—Å–∞–Ω–∏–µ\n",
        "    return describe_garbage_location(image, detections)\n",
        "\n",
        "def test_vlm_on_image(image_path, questions):\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"üì∑ {Path(image_path).name}\")\n",
        "    print('='*60)\n",
        "    \n",
        "    # –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –¥–µ—Ç–µ–∫—Ü–∏—è\n",
        "    img = Image.open(image_path).convert('RGB')\n",
        "    detections = detector.detect(img)\n",
        "    \n",
        "    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è\n",
        "    fig, ax = plt.subplots(1, figsize=(10, 7))\n",
        "    ax.imshow(img)\n",
        "    \n",
        "    for det in detections:\n",
        "        bbox = det['box']\n",
        "        color = COLORS.get(det['label'], 'white')\n",
        "        rect = patches.Rectangle((bbox[0], bbox[1]), bbox[2]-bbox[0], bbox[3]-bbox[1],\n",
        "                                  linewidth=2, edgecolor=color, facecolor='none')\n",
        "        ax.add_patch(rect)\n",
        "        ax.text(bbox[0], bbox[1]-5, f\"{det['label']} {det['confidence']:.0%}\", \n",
        "                color=color, fontsize=9, weight='bold')\n",
        "    \n",
        "    ax.set_title(f\"–î–µ—Ç–µ–∫—Ü–∏–∏: {len(detections)} –æ–±—ä–µ–∫—Ç–æ–≤\")\n",
        "    ax.axis('off')\n",
        "    plt.show()\n",
        "    \n",
        "    # –û–ø–∏—Å–∞–Ω–∏–µ —Å—Ü–µ–Ω—ã\n",
        "    scene = describe_scene(img)\n",
        "    print(f\"\\nüåç –°—Ü–µ–Ω–∞: {scene}\")\n",
        "    \n",
        "    # –û—Ç–≤–µ—Ç—ã –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã\n",
        "    print(\"\\nüí¨ –í–æ–ø—Ä–æ—Å—ã –∏ –æ—Ç–≤–µ—Ç—ã:\")\n",
        "    for q in questions:\n",
        "        answer = answer_question(img, detections, q)\n",
        "        print(f\"\\nQ: {q}\")\n",
        "        print(f\"A: {answer}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6. –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ 3 —Å–ª—É—á–∞–π–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö\n",
        "questions = [\n",
        "    \"What is in this image?\",  # –ß—Ç–æ –Ω–∞ –∫–∞—Ä—Ç–∏–Ω–∫–µ?\n",
        "    \"Is there any plastic in this image?\",\n",
        "    \"Is there any glass?\",\n",
        "    \"How many objects are there?\",\n",
        "]\n",
        "\n",
        "for img_path in random.sample(all_images, min(3, len(all_images))):\n",
        "    test_vlm_on_image(img_path, questions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 7. –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ - –∏–∑–º–µ–Ω–∏—Ç–µ –≤–æ–ø—Ä–æ—Å!\n",
        "image_path = random.choice(all_images)\n",
        "my_question = \"Is there any glass in this image?\"  # <-- –í–∞—à –≤–æ–ø—Ä–æ—Å\n",
        "\n",
        "test_vlm_on_image(image_path, [my_question])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 8. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –Ω–∞ Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!mkdir -p /content/drive/MyDrive/VLM_garbage\n",
        "!cp -r models /content/drive/MyDrive/VLM_garbage/\n",
        "!cp -r vlm_inference /content/drive/MyDrive/VLM_garbage/\n",
        "!cp -r vlm_annotation /content/drive/MyDrive/VLM_garbage/\n",
        "\n",
        "print(\"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –Ω–∞ Google Drive: /MyDrive/VLM_garbage\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
