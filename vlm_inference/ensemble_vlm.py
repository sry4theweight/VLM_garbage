"""
VLM —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∞–Ω—Å–∞–º–±–ª—è –¥–µ—Ç–µ–∫—Ç–æ—Ä–æ–≤ (YOLO + RT-DETR) –∏ LLM

–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:
1. –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ ‚Üí –ê–Ω—Å–∞–º–±–ª—å (YOLO + RT-DETR) ‚Üí –î–µ—Ç–µ–∫—Ü–∏–∏ (bbox, label, confidence)
2. –î–µ—Ç–µ–∫—Ü–∏–∏ + –í–æ–ø—Ä–æ—Å ‚Üí LLM ‚Üí –û—Ç–≤–µ—Ç

–≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤–∞—à–∏ –æ–±—É—á–µ–Ω–Ω—ã–µ –¥–µ—Ç–µ–∫—Ç–æ—Ä—ã –∫–∞–∫ "–≥–ª–∞–∑–∞" –º–æ–¥–µ–ª–∏.
"""

import torch
from PIL import Image
from pathlib import Path
from transformers import AutoTokenizer, AutoModelForCausalLM
import sys
sys.path.append(str(Path(__file__).parent.parent))

from vlm_annotation.ensemble_detector import EnsembleDetector


class EnsembleVLM:
    """VLM –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω—Å–∞–º–±–ª—è –¥–µ—Ç–µ–∫—Ç–æ—Ä–æ–≤ + LLM"""
    
    def __init__(
        self,
        yolo_model_path: str = "models/yolo/yolov8x/best.pt",
        detr_model_path: str = "models/rt-detr/rt-detr-101/m",
        detr_processor_path: str = "models/rt-detr/rt-detr-101/p",
        llm_model: str = "microsoft/phi-2",  # –õ–µ–≥–∫–∞—è LLM
        device: str = None,
        conf_threshold: float = 0.5
    ):
        """
        Args:
            yolo_model_path: –ø—É—Ç—å –∫ YOLO –º–æ–¥–µ–ª–∏
            detr_model_path: –ø—É—Ç—å –∫ RT-DETR –º–æ–¥–µ–ª–∏
            detr_processor_path: –ø—É—Ç—å –∫ RT-DETR –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä—É
            llm_model: LLM –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤
            device: —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ (cuda/cpu)
            conf_threshold: –ø–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–π
        """
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –∞–Ω—Å–∞–º–±–ª—è –¥–µ—Ç–µ–∫—Ç–æ—Ä–æ–≤
        print("Loading ensemble detector...")
        self.detector = EnsembleDetector(
            yolo_model_path=yolo_model_path,
            detr_model_path=detr_model_path,
            detr_processor_path=detr_processor_path,
            device=self.device,
            conf_threshold=conf_threshold
        )
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ LLM
        print(f"Loading LLM: {llm_model}...")
        self.tokenizer = AutoTokenizer.from_pretrained(llm_model, trust_remote_code=True)
        self.llm = AutoModelForCausalLM.from_pretrained(
            llm_model,
            torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
            trust_remote_code=True
        ).to(self.device)
        self.llm.eval()
        
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        print("EnsembleVLM ready!")
    
    def detect_objects(self, image: Image.Image) -> list:
        """–î–µ—Ç–µ–∫—Ü–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏"""
        return self.detector.detect(image)
    
    def format_detections_as_context(self, detections: list) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–µ—Ç–µ–∫—Ü–∏–π –∫–∞–∫ —Ç–µ–∫—Å—Ç–æ–≤—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è LLM"""
        if not detections:
            return "No objects detected in the image."
        
        # –ü–æ–¥—Å—á—ë—Ç –ø–æ –∫–ª–∞—Å—Å–∞–º
        class_counts = {}
        for det in detections:
            label = det['label']
            class_counts[label] = class_counts.get(label, 0) + 1
        
        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–ø–∏—Å–∞–Ω–∏—è
        context_parts = []
        context_parts.append(f"Detected {len(detections)} objects in the image:")
        
        for label, count in class_counts.items():
            context_parts.append(f"- {count} {label} object{'s' if count > 1 else ''}")
        
        # –î–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        context_parts.append("\nDetailed detections:")
        for i, det in enumerate(detections):
            bbox = det['box']
            context_parts.append(
                f"{i+1}. {det['label']} (confidence: {det['confidence']:.0%}) "
                f"at position [{int(bbox[0])}, {int(bbox[1])}, {int(bbox[2])}, {int(bbox[3])}]"
            )
        
        return "\n".join(context_parts)
    
    def answer_question(self, image_path: str, question: str) -> str:
        """
        –û—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é
        
        Args:
            image_path: –ø—É—Ç—å –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é
            question: –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        
        Returns:
            –û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏
        """
        # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        image = Image.open(image_path).convert('RGB')
        
        # –î–µ—Ç–µ–∫—Ü–∏—è –æ–±—ä–µ–∫—Ç–æ–≤
        detections = self.detect_objects(image)
        
        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        context = self.format_detections_as_context(detections)
        
        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è LLM
        prompt = f"""You are a helpful assistant that answers questions about garbage/waste objects in images.

Image Analysis:
{context}

Question: {question}

Answer: """
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
        
        with torch.no_grad():
            outputs = self.llm.generate(
                **inputs,
                max_new_tokens=150,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id
            )
        
        # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞
        full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ –æ—Ç–≤–µ—Ç–∞ (–ø–æ—Å–ª–µ "Answer: ")
        if "Answer: " in full_response:
            answer = full_response.split("Answer: ")[-1].strip()
        else:
            answer = full_response[len(prompt):].strip()
        
        # –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—Å—è –Ω–∞ –ø–µ—Ä–≤–æ–º –ø–µ—Ä–µ–≤–æ–¥–µ —Å—Ç—Ä–æ–∫–∏ –∏–ª–∏ –≤–æ–ø—Ä–æ—Å–µ
        answer = answer.split('\n')[0].strip()
        if '?' in answer:
            answer = answer.split('?')[0] + '.' if answer.split('?')[0] else answer
        
        return answer
    
    def get_detections_info(self, image_path: str) -> dict:
        """–ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–µ—Ç–µ–∫—Ü–∏—è—Ö"""
        image = Image.open(image_path).convert('RGB')
        detections = self.detect_objects(image)
        
        class_counts = {}
        for det in detections:
            label = det['label']
            class_counts[label] = class_counts.get(label, 0) + 1
        
        return {
            "total_objects": len(detections),
            "class_counts": class_counts,
            "detections": detections
        }


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="VLM —Å –∞–Ω—Å–∞–º–±–ª–µ–º –¥–µ—Ç–µ–∫—Ç–æ—Ä–æ–≤")
    parser.add_argument("--image", type=str, required=True, help="–ü—É—Ç—å –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é")
    parser.add_argument("--question", type=str, default="What objects are in this image?")
    parser.add_argument("--yolo_model", type=str, default="models/yolo/yolov8x/best.pt")
    parser.add_argument("--detr_model", type=str, default="models/rt-detr/rt-detr-101/m")
    parser.add_argument("--detr_processor", type=str, default="models/rt-detr/rt-detr-101/p")
    parser.add_argument("--llm_model", type=str, default="microsoft/phi-2")
    parser.add_argument("--conf_threshold", type=float, default=0.5)
    
    args = parser.parse_args()
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
    vlm = EnsembleVLM(
        yolo_model_path=args.yolo_model,
        detr_model_path=args.detr_model,
        detr_processor_path=args.detr_processor,
        llm_model=args.llm_model,
        conf_threshold=args.conf_threshold
    )
    
    # –ü–æ–ª—É—á–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞
    print(f"\nImage: {args.image}")
    print(f"Question: {args.question}")
    
    # –°–Ω–∞—á–∞–ª–∞ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –¥–µ—Ç–µ–∫—Ü–∏–∏
    info = vlm.get_detections_info(args.image)
    print(f"\nüìä Detections: {info['total_objects']} objects")
    for label, count in info['class_counts'].items():
        print(f"   - {label}: {count}")
    
    # –ó–∞—Ç–µ–º –æ—Ç–≤–µ—Ç
    answer = vlm.answer_question(args.image, args.question)
    print(f"\nüí¨ Answer: {answer}")


if __name__ == "__main__":
    main()

