{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "id": "cco2X9AgeRhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "def wandb_colab_login():\n",
        "  \"\"\"Temporary hack to prevent colab from hanging\"\"\"\n",
        "  sys.modules[\"google.colab2\"] = sys.modules[\"google.colab\"]\n",
        "  del sys.modules[\"google.colab\"]\n",
        "  wandb.login()\n",
        "  sys.modules[\"google.colab\"] = sys.modules[\"google.colab2\"]\n",
        "wandb_colab_login()"
      ],
      "metadata": {
        "id": "CDsHYQ6Ot5lL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login(key=\"4e77326bcce901ff230272a5919b12ca4588d281\")"
      ],
      "metadata": {
        "id": "oCTYQprOd_Jk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0XnOrHTqiVJ"
      },
      "source": [
        "### Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gX_B-soMqDCH"
      },
      "outputs": [],
      "source": [
        "!pip install -q git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q git+https://github.com/roboflow/supervision.git\n",
        "!pip install -q accelerate\n",
        "!pip install -q roboflow\n",
        "!pip install -q torchmetrics\n",
        "!pip install -q \"albumentations>=1.4.5\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-BR37YWs6Vd"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5e-XgeZs8al"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import requests\n",
        "\n",
        "import numpy as np\n",
        "import supervision as sv\n",
        "import albumentations as A\n",
        "\n",
        "from PIL import Image\n",
        "from pprint import pprint\n",
        "from roboflow import Roboflow\n",
        "from dataclasses import dataclass, replace\n",
        "from google.colab import userdata\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import (\n",
        "    AutoImageProcessor,\n",
        "    AutoModelForObjectDetection,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
        "from roboflow import Roboflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ihyGXa3Aq00O"
      },
      "outputs": [],
      "source": [
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = AutoModelForObjectDetection.from_pretrained(\"PekingU/rtdetr_v2_r101vd\").to(DEVICE)\n",
        "processor = AutoImageProcessor.from_pretrained(\"PekingU/rtdetr_v2_r101vd\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UEedHjEl5gqh"
      },
      "outputs": [],
      "source": [
        "rf = Roboflow(api_key=\"\")\n",
        "project = rf.workspace(\"kuivashev\").project(\"my-normal-dataset\")\n",
        "version = project.version(1)\n",
        "dataset = version.download(\"coco\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNUoM-C-zH4r"
      },
      "outputs": [],
      "source": [
        "ds_train = sv.DetectionDataset.from_coco(\n",
        "    images_directory_path=f\"{dataset.location}/train\",\n",
        "    annotations_path=f\"{dataset.location}/train/_annotations.coco.json\",\n",
        ")\n",
        "ds_valid = sv.DetectionDataset.from_coco(\n",
        "    images_directory_path=f\"{dataset.location}/valid\",\n",
        "    annotations_path=f\"{dataset.location}/valid/_annotations.coco.json\",\n",
        ")\n",
        "ds_test = sv.DetectionDataset.from_coco(\n",
        "    images_directory_path=f\"{dataset.location}/test\",\n",
        "    annotations_path=f\"{dataset.location}/test/_annotations.coco.json\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "GRID_DIM = 5\n",
        "def _get_labels(anns, class_list):\n",
        "    return [class_list[cid] for cid in anns.class_id]\n",
        "def draw_annotations(frame, anns, class_list,\n",
        "                     box_annotator=None,\n",
        "                     label_annotator=None):\n",
        "    if box_annotator is None:\n",
        "        box_annotator = sv.BoxAnnotator()\n",
        "    if label_annotator is None:\n",
        "        label_annotator = sv.LabelAnnotator(text_scale=1, text_thickness=3)\n",
        "    labels = _get_labels(anns, class_list)\n",
        "    canvas = frame.copy()\n",
        "    canvas = box_annotator.annotate(canvas, anns)\n",
        "    canvas = label_annotator.annotate(canvas, anns, labels=labels)\n",
        "    return canvas\n",
        "def build_grid(dataset, grid_dim=GRID_DIM,\n",
        "               tile_size=(400, 400),\n",
        "               padding_color=sv.Color.WHITE,\n",
        "               margin_color=sv.Color.WHITE):\n",
        "    annotated = []\n",
        "    for idx in range(grid_dim * grid_dim):\n",
        "        _, img, anns = dataset[idx]\n",
        "        annotated.append(draw_annotations(img, anns, dataset.classes))\n",
        "    return sv.create_tiles(\n",
        "        annotated,\n",
        "        grid_size=(grid_dim, grid_dim),\n",
        "        single_tile_size=tile_size,\n",
        "        tile_padding_color=padding_color,\n",
        "        tile_margin_color=margin_color\n",
        "    )"
      ],
      "metadata": {
        "id": "CPLKbRtNukHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T1rf4rDG0tRd"
      },
      "outputs": [],
      "source": [
        "IMAGE_SIZE = 480\n",
        "processor = AutoImageProcessor.from_pretrained(\n",
        "    CHECKPOINT,\n",
        "    do_resize=True,\n",
        "    size={\"width\": IMAGE_SIZE, \"height\": IMAGE_SIZE},\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "def format_to_coco(img_id, class_ids, xyxy_boxes):\n",
        "    ann_list = [\n",
        "        {\n",
        "            \"image_id\": img_id,\n",
        "            \"category_id\": cls,\n",
        "            \"bbox\": [x1, y1, x2 - x1, y2 - y1],\n",
        "            \"area\": (x2 - x1) * (y2 - y1),\n",
        "            \"iscrowd\": 0\n",
        "        }\n",
        "        for cls, (x1, y1, x2, y2) in zip(class_ids, xyxy_boxes)\n",
        "    ]\n",
        "    return {\n",
        "        \"image_id\": img_id,\n",
        "        \"annotations\": ann_list\n",
        "    }\n",
        "\n",
        "class CustomDetectionDataset(Dataset):\n",
        "    def __init__(self, base_dataset, processor, augmentations=None):\n",
        "        self.base_dataset = base_dataset\n",
        "        self.processor = processor\n",
        "        self.augmentations = augmentations\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.base_dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        _, raw_img, raw_ann = self.base_dataset[idx]\n",
        "        img = raw_img[..., ::-1]\n",
        "        boxes = raw_ann.xyxy\n",
        "        labels = raw_ann.class_id\n",
        "        coco_formatted = format_to_coco(idx, labels, boxes)\n",
        "        processed = self.processor(\n",
        "            images=img,\n",
        "            annotations=coco_formatted,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        return {key: tensor.squeeze(0) for key, tensor in processed.items()}"
      ],
      "metadata": {
        "id": "VODMIwUavOye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4RGFftpAEaQC"
      },
      "outputs": [],
      "source": [
        "pytorch_dataset_train = PyTorchDetectionDataset(\n",
        "    ds_train, processor, transform=train_augmentation_and_transform)\n",
        "pytorch_dataset_valid = PyTorchDetectionDataset(\n",
        "    ds_valid, processor, transform=valid_transform)\n",
        "pytorch_dataset_test = PyTorchDetectionDataset(\n",
        "    ds_test, processor, transform=valid_transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OgTqutO9FDM7"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    data = {}\n",
        "    data[\"pixel_values\"] = torch.stack([x[\"pixel_values\"] for x in batch])\n",
        "    data[\"labels\"] = [x[\"labels\"] for x in batch]\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Va2ok5MGZ0R"
      },
      "outputs": [],
      "source": [
        "id2label = dict(enumerate(ds_train.classes))\n",
        "label2id = {v: k for k, v in id2label.items()}\n",
        "@dataclass\n",
        "class ModelOutput:\n",
        "    logits: torch.Tensor\n",
        "    pred_boxes: torch.Tensor\n",
        "class DetectionMAP:\n",
        "    def __init__(self, processor, threshold=0.0, id2label=None):\n",
        "        self.processor = processor\n",
        "        self.threshold = threshold\n",
        "        self.id2label = id2label\n",
        "    def _get_sizes(self, batches):\n",
        "        return [torch.tensor([item[\"size\"] for item in batch]) for batch in batches]\n",
        "    def _extract_targets(self, batches, sizes):\n",
        "        out = []\n",
        "        for batch, size in zip(batches, sizes):\n",
        "            for entry, (h, w) in zip(batch, size):\n",
        "                boxes = sv.xcycwh_to_xyxy(entry[\"boxes\"]) * torch.tensor([w, h, w, h])\n",
        "                out.append({\"boxes\": torch.tensor(boxes), \"labels\": torch.tensor(entry[\"class_labels\"])})\n",
        "        return out\n",
        "    def _extract_preds(self, raw_preds, sizes):\n",
        "        out = []\n",
        "        for logits, scores, boxes in raw_preds:\n",
        "            mo = ModelOutput(logits=torch.tensor(scores), pred_boxes=torch.tensor(boxes))\n",
        "            processed = self.processor.post_process_object_detection(\n",
        "                mo, threshold=self.threshold, target_sizes=sizes\n",
        "            )\n",
        "            out.extend(processed)\n",
        "        return out\n",
        "    @torch.no_grad()\n",
        "    def __call__(self, eval_res):\n",
        "        raw_preds, raw_targs = eval_res.predictions, eval_res.label_ids\n",
        "        sizes = self._get_sizes(raw_targs)\n",
        "        targets = self._extract_targets(raw_targs, sizes)\n",
        "        preds = self._extract_preds(raw_preds, sizes)\n",
        "        m = MeanAveragePrecision(box_format=\"xyxy\", class_metrics=True)\n",
        "        m.warn_on_many_detections = False\n",
        "        m.update(preds, targets)\n",
        "        stats = m.compute()\n",
        "        classes = stats.pop(\"classes\")\n",
        "        maps = stats.pop(\"map_per_class\")\n",
        "        mars = stats.pop(\"mar_100_per_class\")\n",
        "        for cid, mp, mr in zip(classes, maps, mars):\n",
        "            name = self.id2label[cid.item()] if self.id2label else cid.item()\n",
        "            stats[f\"map_{name}\"] = mp\n",
        "            stats[f\"mar_100_{name}\"] = mr\n",
        "        return {k: round(v.item(), 4) for k, v in stats.items()}\n",
        "\n",
        "compute_metrics = DetectionMAP(processor, threshold=0.01, id2label=id2label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7kZr8OtGkIO"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForObjectDetection.from_pretrained(\n",
        "    CHECKPOINT,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id,\n",
        "    anchor_image_size=None,\n",
        "    ignore_mismatched_sizes=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jb2ScVfxIFnt"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=f\"first_tracked_finetune\",\n",
        "    num_train_epochs=30,\n",
        "    max_grad_norm=0.1,\n",
        "    learning_rate=5e-5 * 1.5,\n",
        "    warmup_steps=300,\n",
        "    per_device_train_batch_size=45,\n",
        "    dataloader_num_workers=2,\n",
        "    metric_for_best_model=\"eval_map\",\n",
        "    greater_is_better=True,\n",
        "    load_best_model_at_end=True,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=2,\n",
        "    remove_unused_columns=False,\n",
        "    eval_do_concat_batches=False,\n",
        "    report_to=\"none\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fh5y-l8LJLIR"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=pytorch_dataset_train,\n",
        "    eval_dataset=pytorch_dataset_valid,\n",
        "    tokenizer=processor,\n",
        "    data_collator=collate_fn,\n",
        "    compute_metrics=eval_compute_metrics_fn,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9YVSPgtm6s_I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "379852e3-0227-40ff-eb8c-ceacfe7be25f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngPVGIFew9zC"
      },
      "outputs": [],
      "source": [
        "targets = []\n",
        "predictions = []\n",
        "for entry in ds_test:\n",
        "    file_path, _, annots = entry\n",
        "    img = Image.open(file_path)\n",
        "    tensor_in = processor(img, return_tensors=\"pt\").to(DEVICE)\n",
        "    with torch.no_grad():\n",
        "        model_out = model(**tensor_in)\n",
        "    w, h = img.size\n",
        "    post = processor.post_process_object_detection(\n",
        "        model_out,\n",
        "        target_sizes=[(h, w)],\n",
        "        threshold=0.3\n",
        "    )[0]\n",
        "    dets = sv.Detections.from_transformers(post)\n",
        "    targets.append(annots)\n",
        "    predictions.append(dets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WYxQ46GxxyZb"
      },
      "outputs": [],
      "source": [
        "mean_average_precision = sv.MeanAveragePrecision.from_detections(\n",
        "    predictions=predictions,\n",
        "    targets=targets,\n",
        ")\n",
        "print(f\"map50_95: {mean_average_precision.map50_95:.2f}\")\n",
        "print(f\"map50: {mean_average_precision.map50:.2f}\")\n",
        "print(f\"map75: {mean_average_precision.map75:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CAxMHJ1lP9M2"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"/content/rt-detr/\")\n",
        "processor.save_pretrained(\"/content/rt-detr/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ReJ7Z1xPaJk"
      },
      "outputs": [],
      "source": [
        "NUM_SAMPLES = 35\n",
        "def display_samples(count):\n",
        "    for img_path, original, truth in ds_test[:count]:\n",
        "        pil_img = Image.open(img_path)\n",
        "        batch = processor(pil_img, return_tensors=\"pt\").to(DEVICE)\n",
        "        with torch.no_grad():\n",
        "            out = model(**batch)\n",
        "        w, h = pil_img.size\n",
        "        det_batch = processor.post_process_object_detection(\n",
        "            out, target_sizes=[(h, w)], threshold=0.3\n",
        "        )[0]\n",
        "        preds = sv.Detections.from_transformers(det_batch).with_nms(threshold=0.1)\n",
        "        side_by_side = [\n",
        "            annotate(original, truth, ds_train.classes),\n",
        "            annotate(original, preds, ds_train.classes)\n",
        "        ]\n",
        "        tile_grid = sv.create_tiles(\n",
        "            side_by_side,\n",
        "            titles=['ground truth', 'prediction'],\n",
        "            titles_scale=0.5,\n",
        "            single_tile_size=(400, 400),\n",
        "            tile_padding_color=sv.Color.WHITE,\n",
        "            tile_margin_color=sv.Color.WHITE\n",
        "        )\n",
        "        sv.plot_image(tile_grid, size=(6, 6))\n",
        "\n",
        "display_samples(NUM_SAMPLES)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}