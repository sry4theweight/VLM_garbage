{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rP1yOn0tBQAR"
      },
      "outputs": [],
      "source": [
        "!pip install -q git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q git+https://github.com/roboflow/supervision.git\n",
        "!pip install -q accelerate\n",
        "!pip install -q roboflow\n",
        "!pip install -q torchmetrics\n",
        "!pip install -q \"albumentations>=1.4.5\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZt8UjY4BUJ3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import requests\n",
        "\n",
        "import numpy as np\n",
        "import supervision as sv\n",
        "import albumentations as A\n",
        "\n",
        "from PIL import Image\n",
        "from pprint import pprint\n",
        "from roboflow import Roboflow\n",
        "from dataclasses import dataclass, replace\n",
        "from google.colab import userdata\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import (\n",
        "    AutoImageProcessor,\n",
        "    AutoModelForObjectDetection,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
        "from transformers import RTDetrImageProcessor, RTDetrForObjectDetection, RTDetrV2ForObjectDetection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SyjfdZQaBWjk"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fEPROzdGBY_Y"
      },
      "outputs": [],
      "source": [
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"APIKEY\")\n",
        "project = rf.workspace(\"kuivashev\").project(\"hopefully-final-2zc5r\")\n",
        "version = project.version(1)\n",
        "dataset = version.download(\"coco\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pOZp8miOBaa2"
      },
      "outputs": [],
      "source": [
        "ds_train = sv.DetectionDataset.from_coco(\n",
        "    images_directory_path=f\"{dataset.location}/train\",\n",
        "    annotations_path=f\"{dataset.location}/train/_annotations.coco.json\",\n",
        ")\n",
        "ds_valid = sv.DetectionDataset.from_coco(\n",
        "    images_directory_path=f\"{dataset.location}/valid\",\n",
        "    annotations_path=f\"{dataset.location}/valid/_annotations.coco.json\",\n",
        ")\n",
        "ds_test = sv.DetectionDataset.from_coco(\n",
        "    images_directory_path=f\"{dataset.location}/test\",\n",
        "    annotations_path=f\"{dataset.location}/test/_annotations.coco.json\",\n",
        ")\n",
        "\n",
        "print(f\"Number of training images: {len(ds_train)}\")\n",
        "print(f\"Number of validation images: {len(ds_valid)}\")\n",
        "print(f\"Number of test images: {len(ds_test)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1807GiEcBb4Q"
      },
      "outputs": [],
      "source": [
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = RTDetrV2ForObjectDetection.from_pretrained(\"/content/drive/MyDrive/models/rt-detr/rt-detr-3005/m\", local_files_only=True).to(DEVICE)\n",
        "processor = RTDetrImageProcessor.from_pretrained(\"/content/drive/MyDrive/models/rt-detr/rt-detr-3005/p\", local_files_only=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6sW-DwszBfy2"
      },
      "outputs": [],
      "source": [
        "id2label = {id: label for id, label in enumerate(ds_train.classes)}\n",
        "label2id = {label: id for id, label in enumerate(ds_train.classes)}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TC3XqX47O8N2"
      },
      "outputs": [],
      "source": [
        "!pip install ultralytics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "maErYJxqOv15"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "yolo_model = YOLO(\"/content/drive/MyDrive/models/yolo/yolo_latest/best.pt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTmYD_d4LLba"
      },
      "outputs": [],
      "source": [
        "\n",
        "def annotate(image, annotations, classes):\n",
        "    labels = [\n",
        "        classes[class_id]\n",
        "        for class_id\n",
        "        in annotations.class_id\n",
        "    ]\n",
        "\n",
        "    bounding_box_annotator = sv.BoxAnnotator()\n",
        "    label_annotator = sv.LabelAnnotator(text_scale=1, text_thickness=2)\n",
        "\n",
        "    annotated_image = image.copy()\n",
        "    annotated_image = bounding_box_annotator.annotate(annotated_image, annotations)\n",
        "    annotated_image = label_annotator.annotate(annotated_image, annotations, labels=labels)\n",
        "    return annotated_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3cFppMylDbhh"
      },
      "outputs": [],
      "source": [
        "from re import M\n",
        "import numpy as np\n",
        "from sklearn.isotonic import IsotonicRegression\n",
        "\n",
        "MODEL_WEIGHTS = {\n",
        "    \"YOLOv8x\": {\n",
        "        \"plastic\": 0,#0.958  * 0.916 ,\n",
        "        \"glass\": 0,# 0.932 *  0.961/10,\n",
        "        \"metal\":  0,#0.977 * 0.973  ,\n",
        "        \"paper\": 0,# 0.862 *  0.806,\n",
        "        \"organic\": 0,# 0.727 *  0.644   ,\n",
        "                        \"garb-garbage\":0,\n",
        "\n",
        "    },\n",
        "    \"RT-DETR-101\": {\n",
        "        \"plastic\": 0.8967,\n",
        "        \"glass\": 0.8847,\n",
        "        \"metal\": 0.8705,\n",
        "        \"paper\": 0.7299 ,\n",
        "        \"organic\":  0.6151,\n",
        "                        \"garb-garbage\":0,\n",
        "\n",
        "    }\n",
        "}\n",
        "\"\"\"\n",
        "MODEL_ERRORS = {\n",
        "    \"YOLOv8x\":{\n",
        "        \"plastic\": 1- 0.935 * 0.849 ,\n",
        "        \"glass\": 1- 0.967 *  0.942,\n",
        "        \"metal\":1-  0.955 *  0.93  ,\n",
        "        \"paper\":1- 0.873 *  0.768,\n",
        "        \"organic\": 1- 0.703 *  0.639   ,\n",
        "    },\n",
        "        \"RT-DETR-101\": {\n",
        "        \"plastic\": 1- 0.8967,\n",
        "        \"glass\": 1- 0.8847,\n",
        "        \"metal\": 1- 0.8705,\n",
        "        \"paper\": 1- 0.7299 ,\n",
        "        \"organic\": 1-  0.6151,\n",
        "    }\n",
        "}\n",
        "MODEL_WEIGHTS = {\n",
        "    \"RT-DETR-101\": {\n",
        "        \"plastic\":  MODEL_ERRORS[\"YOLOv8x\"][\"plastic\"] /MODEL_ERRORS[\"RT-DETR-101\"][\"plastic\"]  ,\n",
        "        \"glass\": MODEL_ERRORS[\"YOLOv8x\"][\"glass\"] /MODEL_ERRORS[\"RT-DETR-101\"][\"glass\"] ,\n",
        "        \"metal\": MODEL_ERRORS[\"YOLOv8x\"][\"metal\"] /MODEL_ERRORS[\"RT-DETR-101\"][\"metal\"]   ,\n",
        "        \"paper\": MODEL_ERRORS[\"YOLOv8x\"][\"paper\"] /MODEL_ERRORS[\"RT-DETR-101\"][\"paper\"] ,\n",
        "        \"organic\":MODEL_ERRORS[\"YOLOv8x\"][\"organic\"] /MODEL_ERRORS[\"RT-DETR-101\"][\"organic\"]    ,\n",
        "                \"garb-garbage\":0,\n",
        "\n",
        "    },\n",
        "    \"YOLOv8x\": {\n",
        "        \"plastic\": MODEL_ERRORS[\"RT-DETR-101\"][\"plastic\"] /MODEL_ERRORS[\"YOLOv8x\"][\"plastic\"]  ,\n",
        "        \"glass\":MODEL_ERRORS[\"RT-DETR-101\"][\"glass\"] /MODEL_ERRORS[\"YOLOv8x\"][\"glass\"] ,\n",
        "        \"metal\": MODEL_ERRORS[\"RT-DETR-101\"][\"metal\"] /MODEL_ERRORS[\"YOLOv8x\"][\"metal\"]   ,\n",
        "        \"paper\":MODEL_ERRORS[\"RT-DETR-101\"][\"paper\"] /MODEL_ERRORS[\"YOLOv8x\"][\"paper\"] ,\n",
        "        \"garb-garbage\":0,# 1,\n",
        "        \"organic\": MODEL_ERRORS[\"RT-DETR-101\"][\"organic\"] /MODEL_ERRORS[\"YOLOv8x\"][\"organic\"]    ,\n",
        "    }\n",
        "}\n",
        "\"\"\"\n",
        "CONFIDENCE_DELTA_THRESHOLD = 0.5\n",
        "\n",
        "\n",
        "CALIBRATORS = {\n",
        "    \"YOLOv8x\": {},\n",
        "    \"RT-DETR-101\": {}\n",
        "}\n",
        "\n",
        "def calibrate_confidence(confidence, model_name, class_name):\n",
        "    if model_name not in CALIBRATORS or class_name not in CALIBRATORS[model_name]:\n",
        "        weight = MODEL_WEIGHTS[model_name].get(class_name.lower())\n",
        "        return confidence * np.sqrt(weight)\n",
        "\n",
        "    calibrator = CALIBRATORS[model_name][class_name]\n",
        "    return calibrator.predict([confidence])[0]\n",
        "\n",
        "def calculate_iou(box1, box2):\n",
        "    x1 = max(box1[0], box2[0])\n",
        "    y1 = max(box1[1], box2[1])\n",
        "    x2 = min(box1[2], box2[2])\n",
        "    y2 = min(box1[3], box2[3])\n",
        "\n",
        "    intersection = max(0, x2 - x1) * max(0, y2 - y1)\n",
        "    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
        "    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
        "    union = area1 + area2 - intersection\n",
        "\n",
        "    return intersection / union if union > 0 else 0\n",
        "\n",
        "def match_detections(yolo_detections, detr_detections, iou_threshold=0.3):\n",
        "    matched = []\n",
        "    used_detr = set()\n",
        "\n",
        "    for yolo_idx, yolo_det in enumerate(yolo_detections):\n",
        "        best_match = None\n",
        "        best_iou = 0\n",
        "\n",
        "        for detr_idx, detr_det in enumerate(detr_detections):\n",
        "            if detr_idx in used_detr:\n",
        "                continue\n",
        "\n",
        "            iou = calculate_iou(yolo_det['box'], detr_det['box'])\n",
        "            if iou > iou_threshold and iou > best_iou:\n",
        "                best_iou = iou\n",
        "                best_match = detr_idx\n",
        "\n",
        "        if best_match is not None:\n",
        "            matched.append((yolo_idx, best_match))\n",
        "            used_detr.add(best_match)\n",
        "\n",
        "    return matched\n",
        "\n",
        "def ensemble_predictions(yolo_det, detr_det, class_names):\n",
        "    yolo_label = yolo_det['label']\n",
        "    yolo_conf = yolo_det['confidence']\n",
        "    detr_label = detr_det['label']\n",
        "    detr_conf = detr_det['confidence']\n",
        "\n",
        "    yolo_conf_calibrated = calibrate_confidence(yolo_conf, \"YOLOv8x\", yolo_label)\n",
        "    detr_conf_calibrated = calibrate_confidence(detr_conf, \"RT-DETR-101\", detr_label)\n",
        "\n",
        "    if abs(yolo_conf_calibrated - detr_conf_calibrated) > CONFIDENCE_DELTA_THRESHOLD:\n",
        "        print(\"CONFLICT\")\n",
        "        yolo_weight_for_yolo_class = MODEL_WEIGHTS[\"YOLOv8x\"].get(yolo_label.lower())\n",
        "        detr_weight_for_detr_class = MODEL_WEIGHTS[\"RT-DETR-101\"].get(detr_label.lower())\n",
        "\n",
        "        if yolo_weight_for_yolo_class > detr_weight_for_detr_class:\n",
        "            return yolo_label, yolo_conf_calibrated, yolo_det['box']\n",
        "        else:\n",
        "            return detr_label, detr_conf_calibrated, detr_det['box']\n",
        "\n",
        "    combined_scores = {}\n",
        "\n",
        "    for class_name in class_names:\n",
        "        yolo_score = yolo_conf_calibrated if yolo_label == class_name else 0\n",
        "        detr_score = detr_conf_calibrated if detr_label == class_name else 0\n",
        "        class_yolo_weight = MODEL_WEIGHTS[\"YOLOv8x\"].get(class_name.lower())\n",
        "        class_detr_weight = MODEL_WEIGHTS[\"RT-DETR-101\"].get(class_name.lower())\n",
        "        if class_yolo_weight + class_detr_weight > 0:\n",
        "            combined_scores[class_name] = (class_yolo_weight * yolo_score + class_detr_weight * detr_score) / (class_yolo_weight + class_detr_weight)\n",
        "        else:\n",
        "            combined_scores[class_name] = 0\n",
        "\n",
        "    best_class = max(combined_scores.items(), key=lambda x: x[1])\n",
        "\n",
        "    best_class_name = best_class[0]\n",
        "\n",
        "    yolo_w = MODEL_WEIGHTS[\"YOLOv8x\"].get(best_class_name.lower())\n",
        "    detr_w = MODEL_WEIGHTS[\"RT-DETR-101\"].get(best_class_name.lower())\n",
        "    total_w = yolo_w + detr_w\n",
        "\n",
        "    if total_w > 0:\n",
        "        avg_box = [\n",
        "            yolo_det['box'][0],\n",
        "           yolo_det['box'][1],\n",
        "            yolo_det['box'][2],\n",
        "            yolo_det['box'][3]\n",
        "        ]\n",
        "    else:\n",
        "        avg_box = [\n",
        "            yolo_det['box'][0],\n",
        "           yolo_det['box'][1],\n",
        "            yolo_det['box'][2],\n",
        "            yolo_det['box'][3]\n",
        "        ]\n",
        "\n",
        "    return best_class[0], best_class[1], avg_box\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kio2kWSTfV3Y"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "VERBOSE = str(os.getenv(\"YOLO_VERBOSE\", False)).lower() == \"False\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72lg-F49bUbr"
      },
      "outputs": [],
      "source": [
        "yolo_model.names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KDqpSMnvbWyO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import supervision as sv\n",
        "from PIL import Image\n",
        "\n",
        "targets = []\n",
        "ensemble_preds = []\n",
        "class_names = ['garb-garbage', 'glass', 'metal', 'organic', 'paper', 'plastic']\n",
        "for i in range(45,len(ds_test)):\n",
        "    print(\"________________________________________________________________\")\n",
        "    path, source_image, annotations = ds_test[i]\n",
        "    image = Image.open(path)\n",
        "    sv.plot_image(source_image, size=(6, 6))\n",
        "    inputs = processor(image, return_tensors=\"pt\").to(DEVICE)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    w, h = image.size\n",
        "    results = processor.post_process_object_detection(outputs, target_sizes=[(h, w)], threshold=0.3)\n",
        "    detr_detections = sv.Detections.from_transformers(results[0])\n",
        "\n",
        "    yolo_results = yolo_model(image)\n",
        "    yolo_detections = sv.Detections.from_ultralytics(yolo_results[0])\n",
        "\n",
        "\n",
        "    detr_dets = [{'box': box, 'confidence': conf,\n",
        "                  'label': class_names[cls]}\n",
        "                 for box, conf, cls in zip(detr_detections.xyxy, detr_detections.confidence, detr_detections.class_id)]\n",
        "    yolo_dets = [{'box': box, 'confidence': conf, 'label': class_names[cls]}\n",
        "                 for box, conf, cls in zip(yolo_detections.xyxy, yolo_detections.confidence, yolo_detections.class_id)]\n",
        "\n",
        "    matched = match_detections(yolo_dets, detr_dets)\n",
        "    ensemble_boxes, ensemble_confs, ensemble_classes = [], [], []\n",
        "\n",
        "    for yolo_idx, detr_idx in matched:\n",
        "        label, conf, box = ensemble_predictions(yolo_dets[yolo_idx], detr_dets[detr_idx], class_names)\n",
        "        ensemble_boxes.append(box)\n",
        "        ensemble_confs.append(conf)\n",
        "        ensemble_classes.append(class_names.index(label))\n",
        "\n",
        "    matched_yolo = {m[0] for m in matched}\n",
        "    matched_detr = {m[1] for m in matched}\n",
        "    print(yolo_dets)\n",
        "    print(detr_dets)\n",
        "    for idx, det in enumerate(yolo_dets):\n",
        "        if idx not in matched_yolo:\n",
        "            ensemble_boxes.append(det['box'])\n",
        "            ensemble_confs.append(calibrate_confidence(det['confidence'], \"YOLOv8x\", det['label']))\n",
        "            ensemble_classes.append(class_names.index(det['label']))\n",
        "\n",
        "    for idx, det in enumerate(detr_dets):\n",
        "        if idx not in matched_detr:\n",
        "            ensemble_boxes.append(det['box'])\n",
        "            ensemble_confs.append(calibrate_confidence(det['confidence'], \"RT-DETR-101\", det['label']))\n",
        "            ensemble_classes.append(class_names.index(det['label']))\n",
        "    print(ensemble_boxes)\n",
        "    print(ensemble_classes)\n",
        "    if ensemble_boxes:\n",
        "        ensemble_detection = sv.Detections(\n",
        "            xyxy=np.array(ensemble_boxes),\n",
        "            confidence=np.array(ensemble_confs),\n",
        "            class_id=np.array(ensemble_classes)\n",
        "        )\n",
        "    else:\n",
        "        ensemble_detection = sv.Detections.empty()\n",
        "\n",
        "    targets.append(annotations)\n",
        "    ensemble_preds.append(ensemble_detection)\n",
        "    annotated_images = [\n",
        "        annotate(source_image, annotations, ds_train.classes),\n",
        "        annotate(source_image, ensemble_detection, ds_train.classes)\n",
        "    ]\n",
        "\n",
        "    grid = sv.create_tiles(\n",
        "              annotated_images,\n",
        "              titles=['ground truth', 'prediction'],\n",
        "              titles_scale=0.5,\n",
        "              single_tile_size=(400, 400),\n",
        "              tile_padding_color=sv.Color.WHITE,\n",
        "              tile_margin_color=sv.Color.WHITE\n",
        "          )\n",
        "\n",
        "    sv.plot_image(grid, size=(6, 6))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcaJQVYDjok6"
      },
      "outputs": [],
      "source": [
        "mean_average_precision = sv.MeanAveragePrecision.from_detections(\n",
        "    predictions=ensemble_preds,\n",
        "    targets=targets,\n",
        ")\n",
        "\n",
        "print(f\"map50_95: {mean_average_precision.map50_95:.2f}\")\n",
        "print(f\"map50: {mean_average_precision.map50:.2f}\")\n",
        "print(f\"map75: {mean_average_precision.map75:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WrIb0LBtkjoi"
      },
      "outputs": [],
      "source": [
        "IMAGE_COUNT = 30\n",
        "\n",
        "for i in range(IMAGE_COUNT):\n",
        "    path, sourece_image, annotations = ds_test[i+30]\n",
        "\n",
        "    image = Image.open(path)\n",
        "    inputs = processor(image, return_tensors=\"pt\").to(DEVICE)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    w, h = image.size\n",
        "    results = processor.post_process_object_detection(\n",
        "        outputs, target_sizes=[(h, w)], threshold=0.3)\n",
        "\n",
        "    detections = sv.Detections.from_transformers(results[0]).with_nms(threshold=0.1)\n",
        "\n",
        "    annotated_images = [\n",
        "        annotate(sourece_image, annotations, ds_train.classes),\n",
        "        annotate(sourece_image, detections, ds_train.classes)\n",
        "    ]\n",
        "    grid = sv.create_tiles(\n",
        "        annotated_images,\n",
        "        titles=['ground truth', 'prediction'],\n",
        "        titles_scale=0.5,\n",
        "        single_tile_size=(400, 400),\n",
        "        tile_padding_color=sv.Color.WHITE,\n",
        "        tile_margin_color=sv.Color.WHITE\n",
        "    )\n",
        "    sv.plot_image(grid, size=(6, 6))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
